{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle \n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import vk_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from communities.algorithms import louvain_method, hierarchical_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для записи строки в файл формата pickle\n",
    "def write_string_to_pickle_file(string, file_path):\n",
    "    \"\"\"\n",
    "    Записывает строку в файл формата pickle.\n",
    "    \n",
    "    Аргументы:\n",
    "        string (str): Строка, которую нужно записать в файл.\n",
    "        file_path (str): Путь к файлу, в который нужно записать строку.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(string, f)\n",
    "\n",
    "# Функция для чтения строки из файла формата pickle\n",
    "def read_string_from_pickle_file(file_path):\n",
    "    \"\"\"\n",
    "    Читает строку из файла формата pickle.\n",
    "    \n",
    "    Аргументы:\n",
    "        file_path (str): Путь к файлу, из которого нужно прочитать строку.\n",
    "        \n",
    "    Возвращает:\n",
    "        str: Прочитанная из файла строка.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        string = pickle.load(f)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем строку с токеном из файла формата pickle\n",
    "access_token = read_string_from_pickle_file(\"vk_token_string.pkl\")\n",
    "\n",
    "# Создаем сессию API ВКонтакте с использованием токена\n",
    "vkApiSession= vk_api.VkApi(token=access_token)\n",
    "\n",
    "# Получаем объект API пользователя\n",
    "vkU = vkApiSession.get_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения списка id всех друзей пользователя\n",
    "def get_all_friend_ids(user_id, access_token):\n",
    "    friend_ids = [] # список для хранения id друзей\n",
    "    offset = 0 # смещение для получения следующей порции данных\n",
    "    \n",
    "    # Цикл для получения всех друзей пользователя\n",
    "    while True:\n",
    "        response = requests.get('https://api.vk.com/method/friends.get', params={\n",
    "            'user_id': user_id,\n",
    "            'access_token': access_token,\n",
    "            'count': 1000, # количество друзей, запрашиваемых за один запрос\n",
    "            'offset': offset, # смещение для получения следующей порции данных\n",
    "            'fields': 'id', # запрашиваем только id друзей\n",
    "            'v': '5.131' # версия API ВКонтакте\n",
    "        }).json()\n",
    "        \n",
    "        # Если ответ содержит поле 'response', то добавляем id друзей в список friend_ids\n",
    "        if 'response' in response:\n",
    "            items = response['response']['items']\n",
    "            if len(items) == 0:\n",
    "                break # если список друзей пуст, то выходим из цикла\n",
    "            \n",
    "            for item in items:\n",
    "                friend_ids.append(item['id'])\n",
    "            \n",
    "            offset += 1000 # увеличиваем смещение для получения следующей порции данных\n",
    "        else:\n",
    "            print(response['error']['error_msg']) # выводим сообщение об ошибке\n",
    "            break\n",
    "    \n",
    "    return friend_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения всех друзей пользователя\n",
    "# n_iterations - количество итераций\n",
    "# delay - задержка между запросами\n",
    "# initial_id - идентификатор пользователя, для которого нужно получить всех друзей\n",
    "\n",
    "def get_all_friends(n_iterations, delay, initial_id):\n",
    "\n",
    "    # Создаем множество идентификаторов для парсинга, добавляем начальный идентификатор\n",
    "    set_of_ids_to_parse = set()\n",
    "    set_of_ids_to_parse.add(initial_id)\n",
    "\n",
    "    # Создаем список DataFrame'ов\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Проходимся по всем итерациям\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        counter = 0\n",
    "\n",
    "        # Проходимся по всем идентификаторам в множестве для парсинга\n",
    "        for id in tqdm(set_of_ids_to_parse):\n",
    "\n",
    "            # Задержка между запросами\n",
    "            time.sleep(delay)\n",
    "\n",
    "            # Получаем список идентификаторов друзей пользователя и создаем DataFrame с парами (идентификатор пользователя, идентификатор друга)\n",
    "            list_of_friends = get_all_friend_ids(id, access_token)\n",
    "            list_of_dfs.append(pd.DataFrame(list(zip([id] * len(list_of_friends),  list_of_friends))))\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # Каждые 1000 итераций сохраняем DataFrame в файл\n",
    "            if counter % 1000 == 0:\n",
    "                combined_df = pd.concat(list_of_dfs, axis=0)\n",
    "                combined_df.to_pickle(f\"combined_df_iteration{i}_{counter}.pkl\")\n",
    "                \n",
    "        # Объединяем все DataFrame'ы в один и сохраняем его в файл\n",
    "        combined_df = pd.concat(list_of_dfs, axis=0)\n",
    "        combined_df.to_pickle(f\"combined_df_iteration{i}_full.pkl\")\n",
    "\n",
    "        # Обновляем множество для парсинга - добавляем всех друзей и удаляем уже обработанных пользователей\n",
    "        set_of_ids_to_parse = set()\n",
    "        set_of_ids_to_parse.update(combined_df.iloc[:,1])\n",
    "        set_of_ids_to_parse.difference_update(combined_df.iloc[:,0])\n",
    "\n",
    "    # Объединяем все DataFrame'ы в один и изменяем названия столбцов\n",
    "    combined_df_final = pd.concat(list_of_dfs, axis=0)\n",
    "    combined_df_final.columns = [\"from\", \"to\"]\n",
    "\n",
    "    return combined_df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n",
      " 11%|█         | 7/65 [00:11<01:27,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/65 [00:12<01:21,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User was deleted or banned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/65 [00:17<01:20,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/65 [00:19<01:15,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User was deleted or banned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 16/65 [00:24<01:11,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 17/65 [00:25<01:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 18/65 [00:27<01:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 24/65 [00:36<01:02,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 27/65 [00:41<00:57,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 39/65 [01:00<00:40,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 42/65 [01:05<00:34,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 43/65 [01:06<00:31,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 45/65 [01:09<00:29,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 49/65 [01:16<00:24,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User was deleted or banned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 56/65 [01:27<00:14,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 59/65 [01:32<00:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 62/65 [01:37<00:04,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 64/65 [01:40<00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:41<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This profile is private\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124150763</td>\n",
       "      <td>682502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124150763</td>\n",
       "      <td>1518757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124150763</td>\n",
       "      <td>6099012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124150763</td>\n",
       "      <td>9389942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124150763</td>\n",
       "      <td>9681259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>73311606</td>\n",
       "      <td>674428820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>73311606</td>\n",
       "      <td>694150279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>73311606</td>\n",
       "      <td>706174451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>73311606</td>\n",
       "      <td>740539286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>73311606</td>\n",
       "      <td>784210124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10595 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          from         to\n",
       "0    124150763     682502\n",
       "1    124150763    1518757\n",
       "2    124150763    6099012\n",
       "3    124150763    9389942\n",
       "4    124150763    9681259\n",
       "..         ...        ...\n",
       "287   73311606  674428820\n",
       "288   73311606  694150279\n",
       "289   73311606  706174451\n",
       "290   73311606  740539286\n",
       "291   73311606  784210124\n",
       "\n",
       "[10595 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_id = 124150763\n",
    "N_ITERATIONS = 2\n",
    "DELAY = 1\n",
    "\n",
    "get_all_friends(N_ITERATIONS, DELAY, initial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"combined_df_iteration1_full.pkl\", 'rb') as f:\n",
    "    edge_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования списка в словарь\n",
    "# lst - список, который нужно преобразовать в словарь\n",
    "\n",
    "def list_to_dict(lst):\n",
    "    d = {}\n",
    "    # Проходимся по всем элементам списка\n",
    "    for i in range(len(lst)):\n",
    "        # Добавляем элемент в словарь с ключом, равным строковому представлению элемента, и значением, равным индексу элемента в списке\n",
    "        d[str(lst[i])] = i\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения матрицы смежности из списка ребер\n",
    "# edge_list - список ребер графа в формате [(from_node, to_node), ...]\n",
    "\n",
    "def get_adj_matrix(edge_list):\n",
    "\n",
    "    # Получаем уникальные узлы из списка ребер и сортируем их\n",
    "    nodes = sorted(list(set(edge_list.iloc[:,0]).union(set(edge_list.iloc[:,1]))))\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    # Преобразуем список узлов в словарь, где ключи - строковые представления узлов, а значения - их индексы в списке узлов\n",
    "    ids_to_indeces = list_to_dict(nodes)\n",
    "\n",
    "    # Создаем разреженную матрицу с размером num_nodes x num_nodes и типом данных bool\n",
    "    adjacency_matrix = lil_matrix((num_nodes, num_nodes), dtype=bool)\n",
    "\n",
    "    # Проходимся по всем ребрам графа\n",
    "    for t in tqdm(edge_list.itertuples()):\n",
    "        from_node = t._1\n",
    "        to_node = t._2\n",
    "        from_node_index = ids_to_indeces[str(from_node)]\n",
    "        to_node_index = ids_to_indeces[str(to_node)]\n",
    "        # Устанавливаем значение True в ячейках матрицы, соответствующих соответствующим узлам\n",
    "        adjacency_matrix[from_node_index, to_node_index] = True\n",
    "        adjacency_matrix[to_node_index, from_node_index] = True\n",
    "            \n",
    "    return adjacency_matrix.toarray().astype(np.int8), ids_to_indeces, num_nodes, nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10595it [00:00, 28252.90it/s]\n"
     ]
    }
   ],
   "source": [
    "adj_matrix_int, ids_to_indeces, num_nodes, nodes = get_adj_matrix(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = hierarchical_clustering(adj_matrix_int, linkage = \"complete\", metric = \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: len(x) > 1, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_string_to_pickle_file(res, \"res.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть, алгоритм иерархической кластеризации обнаружил только одно сообщество с более чем 1-й вершиной. Следовательно, задачу обнаружения сообществ в графе можно считать невыполненной. Если бы кластеры получилось выделить, дальнейший проект подразумевал следующие стадии: \n",
    "1. Визуализацию полученных сообществ \n",
    "2. Расчет модулярности графа \n",
    "3. Создание приложения, включающего скачивания данных, выделение сообществ и визуализации \n",
    "\n",
    "\n",
    "Относительно причин, почему не получилось выделить сообщества - можно предположить, что выбранный алгоритм не подходит для работы в этим типом графов (крайне разряженный, степень плотности около 0.004%). Впрочем, другие алгоритмы или подразумевали hard clustering, что также не подходит к задаче выделения сообществ, или работали также плохо."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
